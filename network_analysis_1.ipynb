{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3dfabc-5f04-4c69-970e-b3f6994b8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd65921-2a6c-46b0-99f1-60fbc7f6e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/iasamori/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/users/iasamori/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/users/iasamori/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/users/iasamori/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f918be-ec2b-434a-915f-bbb583f45201",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee66f460-7e65-48d6-a589-1a4878c917a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    words = word_tokenize(text, language='english', preserve_line=True)\n",
    "    # words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e679fa-3917-4db8-8720-66d967046d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_custom_removal(text, frequent_words):\n",
    "    words = preprocess_text(text)\n",
    "    # Remove additional frequent words\n",
    "    words = [word for word in words if word not in frequent_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd8ec49-468b-4064-b4e9-3348871d3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(corpus):\n",
    "    preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "    \n",
    "    all_words = [word for doc in preprocessed_corpus for word in doc]\n",
    "    word_counts = Counter(all_words)\n",
    "    threshold = 100000 \n",
    "    frequent_words = {word for word, count in word_counts.items() if count > threshold}\n",
    "    \n",
    "    filtered_corpus = [preprocess_text_with_custom_removal(doc, frequent_words) for doc in corpus]\n",
    "\n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b71eb1-e090-4f46-9c1e-45ba47654a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(corpus):\n",
    "    filtered_corpus = filter_corpus(corpus)\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(filtered_corpus)\n",
    "\n",
    "    co_occurrence_matrix = (X.T @ X).toarray()\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            if i != j and co_occurrence_matrix[i, j] > 1:  # Avoid self-loops and zero edges\n",
    "                # Convert weight to int to avoid JSON serialization errors\n",
    "                weight = int(co_occurrence_matrix[i, j])\n",
    "                G.add_edge(word1, word2, weight=weight)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96713da4-6609-49a2-91a2-3a9eeb38abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_community_centrality_df(communities, degree_centrality):\n",
    "\n",
    "    community_degree_centrality = {}\n",
    "    \n",
    "    for i, community in enumerate(communities):\n",
    "        community_degree_centrality[f\"community_{i+1}\"] = {}\n",
    "        community_terms = list(community)\n",
    "        for term in community_terms:\n",
    "            community_degree_centrality[f\"community_{i+1}\"][term] = degree_centrality[term]\n",
    "    \n",
    "        community_degree_centrality[f\"community_{i+1}\"] = dict(sorted(community_degree_centrality[f\"community_{i+1}\"].items(),\n",
    "                                                                      key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for community, terms_dict in community_degree_centrality.items():\n",
    "        for term, value in terms_dict.items():\n",
    "            rows.append({'community': community, 'term': term, 'value': value})\n",
    "    \n",
    "    community_degree_centrality_df = pd.DataFrame(rows)\n",
    "\n",
    "    return community_degree_centrality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a6b775-b4b8-4960-ac54-ed7d12ba5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./combined_comments_2.csv')\n",
    "data = data.dropna(subset=['create_time'])\n",
    "data = data[(data['create_time'] < 1e10) & (data['create_time'] > 0)]\n",
    "\n",
    "datetime_list = pd.to_datetime(data['create_time'], unit='s', errors='coerce')\n",
    "month_year = [f\"{i.month}_{i.year}\" for i in datetime_list]\n",
    "data['month_year'] = month_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a49ecd7-2038-49c1-9f8d-8b06dd09258a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month_year\n",
       "4_2024     51322\n",
       "7_2023     44678\n",
       "8_2023      6267\n",
       "5_2024      3487\n",
       "9_2023      1468\n",
       "6_2024      1377\n",
       "8_2024       955\n",
       "10_2023      592\n",
       "7_2024       466\n",
       "11_2023      322\n",
       "1_2024       264\n",
       "9_2024       237\n",
       "12_2023      187\n",
       "2_2024       184\n",
       "10_2024       87\n",
       "3_2024        20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['month_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e69f9c9-6237-43cc-b8ee-671e54a476a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month_year\n",
       "4_2024     28671\n",
       "5_2024      1853\n",
       "8_2024       559\n",
       "6_2024       509\n",
       "7_2024       287\n",
       "9_2024       101\n",
       "10_2024       24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['month_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428a1625-28ab-45e3-96e0-30932588903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [11:40<00:00, 43.80s/it]\n"
     ]
    }
   ],
   "source": [
    "month_year_list = list(data['month_year'].unique())\n",
    "\n",
    "for month_year in tqdm(month_year_list):\n",
    "    month_year_df = data[data['month_year'] == month_year]\n",
    "    month_text = month_year_df['text'].dropna()\n",
    "    month_text = list(month_text.values)\n",
    "\n",
    "    G = get_graph(month_text)\n",
    "    \n",
    "    communities = list(greedy_modularity_communities(G))\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    \n",
    "    with open(f'./network_analysis_data/{month_year}_communities.txt', 'w') as f:\n",
    "        for i, community in enumerate(communities):\n",
    "            f.write(f\"Community {i+1}: {', '.join(community)}\\n\\n\")\n",
    "    \n",
    "    community_degree_centrality_df = generate_community_centrality_df(communities, degree_centrality)\n",
    "    community_degree_centrality_df.to_csv(f\"./network_analysis_data/{month_year}_community_degree_centrality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "904067f1-9057-4c43-a859-3c1a896729d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = get_graph(data['text'].dropna())\n",
    "    \n",
    "communities = list(greedy_modularity_communities(G))\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "with open(f'./network_analysis_data/all_data_communities.txt', 'w') as f:\n",
    "    for i, community in enumerate(communities):\n",
    "        f.write(f\"Community {i+1}: {', '.join(community)}\\n\\n\")\n",
    "\n",
    "community_degree_centrality_df = generate_community_centrality_df(communities, degree_centrality)\n",
    "community_degree_centrality_df.to_csv(f\"./network_analysis_data/all_data_community_degree_centrality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14245f90-a1b9-4848-9b1d-e69b9dc8623d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m G \u001b[38;5;241m=\u001b[39m get_graph(texts)\n\u001b[1;32m      3\u001b[0m communities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(greedy_modularity_communities(G))\n\u001b[1;32m      4\u001b[0m degree_centrality \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mdegree_centrality(G)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "G = get_graph(texts)\n",
    "\n",
    "communities = list(greedy_modularity_communities(G))\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "with open(f'./network_analysis_data/{month_year}_communities.txt', 'w') as f:\n",
    "    for i, community in enumerate(communities):\n",
    "        f.write(f\"Community {i+1}: {', '.join(community)}\\n\\n\")\n",
    "\n",
    "community_degree_centrality_df = generate_community_centrality_df(communities, degree_centrality)\n",
    "community_degree_centrality_df.to_csv(f\"./network_analysis_data/{month_year}_community_degree_centrality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee324ac4-a4ae-4f76-be0b-dc46fb4869fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_env",
   "language": "python",
   "name": "esm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
